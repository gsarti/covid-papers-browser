{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "create_db_entries_pickles.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPdV7Ar-pVaA",
        "colab_type": "text"
      },
      "source": [
        "# Create CORD19 MongoDB Database Entries\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gsarti/covid-paper-browser/blob/master/notebooks/create_db_entries_pickles.ipynb)\n",
        "\n",
        "This notebook is intended to be run on Google Colab or other GPU providers platforms as a step-by-step replacement for the `create_db` script that can be run locally and don't require excessive amounts of time.\n",
        "\n",
        "**Disclaimer**: The procedure requires having access to a Google Drive account with at least 13 GB of storage available. The total runtime in Colab is approximately 2 h 45 minutes.\n",
        "\n",
        "## Steps\n",
        "\n",
        "1. Open this notebook in Colab by clicking the button above.\n",
        "\n",
        "2. Connect to a GPU runtime and mount your Drive (on the left, Files -> Mount Drive). You should see the path `drive/My Drive` containing your files on the left section.\n",
        "\n",
        "3. Run the cells below. Those will download data, install Python libraries, define methods and ultimately run the `create_database` method. Instead of actually creating the database, this saves the database entries in `X` files named `db_entriesX.pkl` on your Google Drive. The total size is approx 12.45 GB for the full entries (including both `title_abstract_embeddings` and `paragraphs_embeddings`), or 2.3 GB if you decide to compute only `title_abstract_embeddings`. The latter are also way faster (11 min 40 sec for me).\n",
        "\n",
        "4. Download the pickled files on your PC.\n",
        "\n",
        "5. Run a MongoDB session in the background, open a Python session in the same folder of the downloaded files and run:\n",
        "\n",
        "```python\n",
        "import os\n",
        "from pymongo import MongoClient\n",
        "\n",
        "YOUR_DB_NAME = 'coviddb'\n",
        "YOUR_COLLECTION_NAME = 'cord19scibert'\n",
        "\n",
        "files = [f for f in os.listdir() if f.startswith('db_entries')]\n",
        "client = MongoClient()\n",
        "db = client[YOUR_DB_NAME]\n",
        "col = db[YOUR_COLLECTION_NAME]\n",
        "for fname in tqdm(files):\n",
        "    with open(fname, 'rb') as f:\n",
        "        entries = pickle.load(f)\n",
        "    col.insert_many(entries)\n",
        "```\n",
        "\n",
        "Notice that doing this may cannibalize your resources (it did with me on a 16GB RAM machine), so consider doing this by loading and inserting one pickled file at a time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMW-IH8bwddy",
        "colab_type": "text"
      },
      "source": [
        "## Download Data from AI2 Servers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4jMQZiQu4l5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash capture\n",
        "mkdir data\n",
        "\n",
        "DATE=2020-03-27\n",
        "DATA_DIR=data\n",
        "\n",
        "wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/\"${DATE}\"/comm_use_subset.tar.gz -P \"${DATA_DIR}\"\n",
        "wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/\"${DATE}\"/noncomm_use_subset.tar.gz -P \"${DATA_DIR}\"\n",
        "wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/\"${DATE}\"/custom_license.tar.gz -P \"${DATA_DIR}\"\n",
        "wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/\"${DATE}\"/biorxiv_medrxiv.tar.gz -P \"${DATA_DIR}\"\n",
        "wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/\"${DATE}\"/metadata.csv -P \"${DATA_DIR}\"\n",
        "\n",
        "tar -zxvf \"${DATA_DIR}\"/comm_use_subset.tar.gz -C \"${DATA_DIR}\"\n",
        "tar -zxvf \"${DATA_DIR}\"/noncomm_use_subset.tar.gz -C \"${DATA_DIR}\"\n",
        "tar -zxvf \"${DATA_DIR}\"/custom_license.tar.gz -C \"${DATA_DIR}\"\n",
        "tar -zxvf \"${DATA_DIR}\"/biorxiv_medrxiv.tar.gz -C \"${DATA_DIR}\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SrfClq0wlD0",
        "colab_type": "text"
      },
      "source": [
        "## Install Python Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nKJBz3JGyV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install -U transformers pandas sentence_transformers tqdm compress_pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBNGOlN8xAtx",
        "colab_type": "text"
      },
      "source": [
        "## Define the PaperDatabaseEntry Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFtFYSo7xdO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PaperDatabaseEntry:\n",
        "    \"\"\" Defines the Paper object stored in the database. \"\"\"\n",
        "    def __init__(self, x):\n",
        "        self.cord_id = x['cord_uid']\n",
        "        self.url = x['url']\n",
        "        self.sha = x['sha'].split(';')[0]\n",
        "        self.title = x['title'] if x['title'] not in FILTER_TITLES else ''\n",
        "        self.source = x['source_x']\n",
        "        self.doi = x['doi']\n",
        "        self.pmc_id = x['pmcid']\n",
        "        self.pubmed_id = x['pubmed_id']\n",
        "        self.license = x['license']\n",
        "        self.abstract = x['abstract'] if x['abstract'] not in FILTER_ABSTRACTS else ''\n",
        "        self.publish_time = x['publish_time']\n",
        "        self.authors = x['authors'].split('; ')\n",
        "        self.journal = x['journal']\n",
        "        self.microsoft_id = x['Microsoft Academic Paper ID']\n",
        "        self.who_id = x['WHO #Covidence']\n",
        "        self.paragraphs = [] # List of tuples (section_name, text)\n",
        "        self.bibliography = [] # List of dictionaries\n",
        "        self.title_abstract_embeddings = []\n",
        "        self.paragraphs_embeddings = []\n",
        "\n",
        "    def as_dict(self):\n",
        "        return {\n",
        "            'cord_id': self.cord_id,\n",
        "            'url': self.url,\n",
        "            'sha': self.sha,\n",
        "            'title': self.title,\n",
        "            'source': self.source,\n",
        "            'doi': self.doi,\n",
        "            'pmc_id': self.pmc_id,\n",
        "            'pubmed_id': self.pubmed_id,\n",
        "            'license': self.license,\n",
        "            'abstract': self.abstract,\n",
        "            'publish_time': self.publish_time,\n",
        "            'authors': self.authors,\n",
        "            'journal': self.journal,\n",
        "            'microsoft_id': self.microsoft_id,\n",
        "            'who_id': self.who_id,\n",
        "            'paragraphs': self.paragraphs,\n",
        "            'bibliography': self.bibliography,\n",
        "            'title_abstract_embeddings': self.title_abstract_embeddings,\n",
        "            'paragraphs_embeddings': self.paragraphs_embeddings,\n",
        "        }\n",
        "\n",
        "    def compute_title_abstract_embeddings(self, model):\n",
        "        if self.title != '' or self.abstract != '':\n",
        "            title_abstract = self.title + ' ' + self.abstract\n",
        "            embedding = model.encode([title_abstract], show_progress_bar=False)\n",
        "            self.title_abstract_embeddings = embedding[0].tolist()\n",
        "    \n",
        "    def compute_paragraphs_embeddings(self, model):\n",
        "        if len(self.paragraphs) > 0:\n",
        "            paragraphs_text = [tup[1] for tup in self.paragraphs]\n",
        "            paragraph_embeddings = model.encode(paragraphs_text, show_progress_bar=False)\n",
        "            self.paragraphs_embeddings = [e.tolist() for e in paragraph_embeddings]\n",
        "\n",
        "FILTER_TITLES = ['Index', 'Subject Index', 'Subject index', 'Author index', 'Contents', \n",
        "        'Articles of Significant Interest Selected from This Issue by the Editors',\n",
        "        'Information for Authors', 'Graphical contents list', 'Table of Contents',\n",
        "        'In brief', 'Preface', 'Editorial Board', 'Author Index',\n",
        "        'Volume Contents', 'Research brief', 'Abstracts', 'Keyword index',\n",
        "        'In This Issue', 'Department of Error', 'Contents list', 'Highlights of this issue',\n",
        "        'Abbreviations', 'Introduction', 'Cumulative Index', 'Positions available',\n",
        "        'Index of Authors', 'Editorial', 'Journal Watch', 'QUIZ CORNER', 'Foreword', 'Table of contents',\n",
        "        'Quiz Corner', 'INDEX', 'Bibliography of the current world literature',\n",
        "        'Index of Subjects', '60 Seconds', 'Contributors',\n",
        "        'Public Health Watch', 'Commentary', 'Chapter 1 Introduction',\n",
        "        'Facts and ideas from anywhere', 'Erratum', 'Contents of Volume', 'Patent reports',\n",
        "        'Oral presentations', 'Abk√ºrzungen', 'Abstracts cont.', 'Related elsevier virology titles contents alert',\n",
        "        'Keyword Index', 'Volume contents', 'Articles of Significant Interest in This Issue',\n",
        "        'Appendix', 'Abk√ºrzungsverzeichnis', 'List of Abbreviations', 'Editorial Board and Contents',\n",
        "        'Instructions for Authors', 'Corrections', 'II. Sachverzeichnis', '1 Introduction',  'List of abbreviations',\n",
        "        'Response', 'Feedback', 'Poster Sessions', 'News Briefs', 'Commentary on the Feature Article',\n",
        "        'Papers to Appear in Forthcoming Issues', 'TOC', 'Glossary', 'Letter from the editor', 'Croup',\n",
        "        'Acronyms and Abbreviations', 'Highlights', 'Forthcoming papers', 'Poster presentations', 'Authors',\n",
        "        'Journal Roundup', 'Index of authors', 'Table des mots-cl√©s', 'Posters', 'Cumulative Index 2004', \n",
        "        'A Message from the Editor', 'Contents and Editorial Board', 'SUBJECT INDEX', 'Contents page 1',\n",
        "]\n",
        "\n",
        "FILTER_ABSTRACTS = ['Unknown', '[Image: see text]']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFtEsGLvyODW",
        "colab_type": "text"
      },
      "source": [
        "## Methods to Load Model and Rank Abstracts/Paragraphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaKsgCl-ADwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist\n",
        "from sentence_transformers import models, SentenceTransformer\n",
        "\n",
        "\n",
        "def load_sentence_transformer(\n",
        "    name: str = 'gsarti/scibert-nli', \n",
        "    max_seq_length: int  = 128, \n",
        "    do_lower_case: bool  = True) -> SentenceTransformer:\n",
        "    \"\"\" Loads a SentenceTransformer from HuggingFace AutoModel bestiary \"\"\"\n",
        "    word_embedding_model = models.BERT(\n",
        "            'gsarti/scibert-nli',\n",
        "            max_seq_length=128,\n",
        "            do_lower_case=True\n",
        "        )\n",
        "    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
        "            pooling_mode_mean_tokens=True,\n",
        "            pooling_mode_cls_token=False,\n",
        "            pooling_mode_max_tokens=False\n",
        "        )\n",
        "    return SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
        "\n",
        "\n",
        "def match_query(\n",
        "    query: str,\n",
        "    model: SentenceTransformer,\n",
        "    corpus: list,\n",
        "    corpus_embed: list,\n",
        "    top_k: int = 5) -> list:\n",
        "    \"\"\" Matches query and paragraph embeddings, returning top scoring paragraphs ids and scores \"\"\"\n",
        "    query_embed = model.encode([query], show_progress_bar=False)[0].reshape(1,-1)\n",
        "    distances = 1 - cdist(query_embed, corpus_embed, \"cosine\")\n",
        "    results = zip(corpus, distances.reshape(-1,1))\n",
        "    results = sorted(results, key=lambda x: x[1], reverse=True)\n",
        "    return results[:top_k]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2QrpppFytiv",
        "colab_type": "text"
      },
      "source": [
        "## Methods to Create Database Entries and Pickle Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbm_Yv_k_et2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def create_db_entry(\n",
        "    csv_entry: dict, \n",
        "    model: SentenceTransformer,\n",
        "    do_paragraph_embeddings: bool) -> PaperDatabaseEntry:\n",
        "    \"\"\" Creates a single DB entry from a csv entry using the model for creating embeddings \"\"\"\n",
        "    db_entry = PaperDatabaseEntry(csv_entry)\n",
        "    db_entry.compute_title_abstract_embeddings(model)\n",
        "    if csv_entry['has_full_text'] == True:\n",
        "        foldername = csv_entry['full_text_file']\n",
        "        # Format is e.g. 'data/biorxiv_medrxiv/file.json'\n",
        "        path = os.path.join('data', foldername, f'{db_entry.sha}.json')\n",
        "        file = json.load(open(path, 'r'))\n",
        "        paragraphs = []\n",
        "        # Order is: abstracts, body, back_matter, ref_entries\n",
        "        parts = [file['abstract'], file['body_text'], file['back_matter']]\n",
        "        for part in parts:\n",
        "            for paragraph in part:\n",
        "                paragraphs.append((paragraph['section'], paragraph['text']))\n",
        "        for key, paragraph in file['ref_entries'].items():\n",
        "            paragraphs.append((paragraph['type'].title(), paragraph['text']))\n",
        "        db_entry.paragraphs = paragraphs\n",
        "        if do_paragraph_embeddings:\n",
        "            db_entry.compute_paragraphs_embeddings(model)\n",
        "        db_entry.bibliography = [file['bib_entries'][entry] for entry in file['bib_entries']]\n",
        "    return db_entry\n",
        "\n",
        "def create_pickles(\n",
        "    model_name: str = 'gsarti/scibert-nli',\n",
        "    data_path: str = 'data/metadata.csv',\n",
        "    n_batches: int = 1,\n",
        "    do_paragraph_embeddings: bool = True) -> None:\n",
        "    \"\"\" Creates a new Mongo database with entries from data_path, using model model_name \"\"\"\n",
        "    model = load_sentence_transformer(model_name)\n",
        "    df = pd.read_csv(data_path)\n",
        "    df = df.fillna('')\n",
        "    df_batches = np.array_split(df, n_batches)\n",
        "    inserted = 0\n",
        "    for i, batch in enumerate(df_batches):\n",
        "        if i == 0:\n",
        "          continue\n",
        "        print(f'Batch {i}')\n",
        "        db_entries = []\n",
        "        for _, row in tqdm(batch.iterrows()):\n",
        "            db_entry = create_db_entry(row, model, do_paragraph_embeddings)\n",
        "            # Only add entries with at least one between title and abstract to enable search\n",
        "            if len(db_entry.title_abstract_embeddings) > 0:\n",
        "                db_entries.append(db_entry.as_dict())\n",
        "        print('Saving entries to', f'drive/My Drive/db_entries{i}.pkl')\n",
        "        with open(f'drive/My Drive/db_entries{i}.pkl', 'wb') as f:\n",
        "            pickle.dump(db_entries, f)\n",
        "        inserted += len(db_entries)\n",
        "        print(f'Inserted {len(db_entries)} new entries.')\n",
        "    print(f'Done. {len(df)} processed, {inserted} inserted.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F07UKeqxy0RG",
        "colab_type": "text"
      },
      "source": [
        "## Run the Creation of Pickle Files\n",
        "\n",
        "This is the part that will take approximately 3 hours to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ug1sJ691u6l-",
        "colab_type": "code",
        "outputId": "590b47a8-6162-4b8b-a789-77c7496170a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "create_pickles(model=model, data_path='data/metadata.csv', n_batches=5, do_paragraph_embeddings=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9155it [33:11,  4.60it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving entries to drive/My Drive/db_entries1.pkl\n",
            "Inserted 9147 new entries.\n",
            "Batch 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9155it [12:18, 12.40it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving entries to drive/My Drive/db_entries2.pkl\n",
            "Inserted 9125 new entries.\n",
            "Batch 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9155it [50:16,  3.03it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving entries to drive/My Drive/db_entries3.pkl\n",
            "Inserted 8918 new entries.\n",
            "Batch 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9154it [35:26,  4.31it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving entries to drive/My Drive/db_entries4.pkl\n",
            "Inserted 9100 new entries.\n",
            "Done. Inserted 45774 total new entries.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMmWY8c6zL0y",
        "colab_type": "text"
      },
      "source": [
        "## Check that Files were Generated Correctly\n",
        "\n",
        "You should see roughly 10 GB here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibWC3xN3CAaP",
        "colab_type": "code",
        "outputId": "714c3aef-af55-488b-d940-05ef98fa2947",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!du -h \"drive/My Drive/db_entries\"*"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.2G\tdrive/My Drive/db_entries0.pkl\n",
            "2.4G\tdrive/My Drive/db_entries1.pkl\n",
            "817M\tdrive/My Drive/db_entries2.pkl\n",
            "3.7G\tdrive/My Drive/db_entries3.pkl\n",
            "2.5G\tdrive/My Drive/db_entries4.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}